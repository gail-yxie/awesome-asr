<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"TAC: Timestamped Audio Captioning","children":[{"content":"Problem &amp; Motivation","children":[{"content":"Supervision Mismatch","children":[{"content":"Existing datasets pair 10-30s clips with single global captions","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Causes &quot;semantic collapse&quot;: temporal details compressed into brief summaries","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"Global pooling supervision collapses temporal information","children":[],"payload":{"tag":"li","lines":"5,7"}}],"payload":{"tag":"h3","lines":"2,3"}},{"content":"Current LALM Limitations","children":[{"content":"Struggle to disentangle overlapping events in complex acoustic scenes","children":[],"payload":{"tag":"li","lines":"8,9"}},{"content":"Temporally inconsistent descriptions","children":[],"payload":{"tag":"li","lines":"9,10"}},{"content":"High hallucination rates (10-12% in existing models)","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"Cannot produce fine-grained temporal grounding","children":[],"payload":{"tag":"li","lines":"11,13"}}],"payload":{"tag":"h3","lines":"7,8"}},{"content":"Application Needs","children":[{"content":"Safety-critical monitoring requires precise event localization","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Accessibility tools need accurate temporal descriptions","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"Multimodal reasoning requires rich, structured audio representations","children":[],"payload":{"tag":"li","lines":"16,19"}}],"payload":{"tag":"h3","lines":"13,14"}}],"payload":{"tag":"h2","lines":"1,2"}},{"content":"Architecture / Method","children":[{"content":"Dynamic Acoustic Mixer (Data Pipeline)","children":[{"content":"Scene Templates","children":[{"content":"Structural specifications defining temporal constraints and role bindings","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"Roles: speech, music, sound effects, background noise","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Generates infinite synthetic mixtures from real audio sources","children":[],"payload":{"tag":"li","lines":"24,26"}}],"payload":{"tag":"h4","lines":"21,22"}},{"content":"RMS-based Activity Detection","children":[{"content":"Precise temporal grounding via loudness proxies","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Timestamps derived from individual stems before mixing","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"&quot;Analysis by synthesis&quot;: ground truth from construction","children":[],"payload":{"tag":"li","lines":"29,31"}}],"payload":{"tag":"h4","lines":"26,27"}},{"content":"Dynamic Supervision","children":[{"content":"Varying merge thresholds (event fusion distance)","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Varying activity thresholds (minimum loudness sensitivity)","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Diverse supervision schedules during training","children":[],"payload":{"tag":"li","lines":"34,36"}}],"payload":{"tag":"h4","lines":"31,32"}}],"payload":{"tag":"h3","lines":"20,21"}},{"content":"Multitask Prompts &amp; Output","children":[{"content":"Four Controllable Properties","children":[{"content":"Style: keywords, brief, or detailed descriptions","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"Merge Threshold: determines event fusion distance","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"Activity Threshold: controls sound loudness sensitivity","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Time Resolution: rounding precision for timestamps","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"h4","lines":"37,38"}},{"content":"Structured Output Format","children":[{"content":"Token-efficient concatenation of event labels with timestamps","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"Synchronized temporal grounding for each event","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"Enables reliable downstream parsing","children":[],"payload":{"tag":"li","lines":"46,48"}}],"payload":{"tag":"h4","lines":"43,44"}}],"payload":{"tag":"h3","lines":"36,37"}},{"content":"TAC Model","children":[{"content":"Backbone","children":[{"content":"Qwen2-Audio (frozen weights)","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"LoRA fine-tuning (rank 128, ~22M trainable parameters)","children":[],"payload":{"tag":"li","lines":"51,53"}}],"payload":{"tag":"h4","lines":"49,50"}},{"content":"Pre-training","children":[{"content":"Continued on high-fidelity single-source audio","children":[],"payload":{"tag":"li","lines":"54,55"}},{"content":"LLM-generated captions for clean event-caption associations","children":[],"payload":{"tag":"li","lines":"55,57"}}],"payload":{"tag":"h4","lines":"53,54"}},{"content":"Loss Function","children":[{"content":"Weighted combination: L_total = L_LM + &#x3bb;_time * &#x3a3; CE(y_t, &#x177;_t)","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"&#x3bb;_time = 5.0 for optimal timestamp precision","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Extra emphasis on timestamp tokens","children":[],"payload":{"tag":"li","lines":"60,62"}}],"payload":{"tag":"h4","lines":"57,58"}},{"content":"Training","children":[{"content":"5,000 iterations on 8 NVIDIA A100 GPUs","children":[],"payload":{"tag":"li","lines":"63,64"}},{"content":"Speech segments processed through Whisper for transcription","children":[],"payload":{"tag":"li","lines":"64,66"}}],"payload":{"tag":"h4","lines":"62,63"}}],"payload":{"tag":"h3","lines":"48,49"}},{"content":"TAC-V: Audiovisual Extension","children":[{"content":"Five-Stage Pipeline","children":[{"content":"1. Audio processing via TAC + Whisper speech transcription","children":[],"payload":{"tag":"li","lines":"68,69","listIndex":1}},{"content":"2. Event confidence scoring using FLAM (contrastive audio-text model)","children":[],"payload":{"tag":"li","lines":"69,70","listIndex":2}},{"content":"3. Timestamped shot-list creation from audio events","children":[],"payload":{"tag":"li","lines":"70,71","listIndex":3}},{"content":"4. Visual frame analysis at configurable rates (22fps)","children":[],"payload":{"tag":"li","lines":"71,72","listIndex":4}},{"content":"5. VLM-based hallucination correction and visual grounding","children":[],"payload":{"tag":"li","lines":"72,74","listIndex":5}}],"payload":{"tag":"h4","lines":"67,68"}},{"content":"Design Philosophy","children":[{"content":"Late modality fusion to avoid cross-modal hallucination","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"Audio and visual streams processed independently then reconciled","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Leverages best-in-class models for each modality","children":[],"payload":{"tag":"li","lines":"77,80"}}],"payload":{"tag":"h4","lines":"74,75"}}],"payload":{"tag":"h3","lines":"66,67"}}],"payload":{"tag":"h2","lines":"19,20"}},{"content":"Evaluation","children":[{"content":"Metrics","children":[{"content":"Semantic Alignment","children":[{"content":"LLM-based judge with bipartite matching (predicted vs. ground truth events)","children":[],"payload":{"tag":"li","lines":"83,85"}}],"payload":{"tag":"h4","lines":"82,83"}},{"content":"Temporal Precision","children":[{"content":"Segment-Based F1 (SegF1): 100ms resolution activity detection","children":[],"payload":{"tag":"li","lines":"86,87"}},{"content":"Event-Based F1 (EvtF1): discrete event evaluation with &#xb1;1.0s collar","children":[],"payload":{"tag":"li","lines":"87,89"}}],"payload":{"tag":"h4","lines":"85,86"}},{"content":"Robustness","children":[{"content":"Hallucination Rate: % of events below FLAM confidence threshold (&#x3c4;=0.25)","children":[],"payload":{"tag":"li","lines":"90,91"}},{"content":"Confidence: audio-text similarity score","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"Specificity: accuracy of duration descriptions","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"h4","lines":"89,90"}}],"payload":{"tag":"h3","lines":"81,82"}},{"content":"Dense Captioning (TACOS Benchmark)","children":[{"content":"Event F1: 0.50 (vs. Qwen3-Omni: 0.37, Audio Flamingo 3: 0.27)","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"Segment F1: 0.71 (outperforming all competitors)","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Hallucination Rate: 4.9% (vs. Audio Flamingo 3: 11.6%)","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Confidence: 0.89 | Specificity: 0.74","children":[],"payload":{"tag":"li","lines":"98,100"}}],"payload":{"tag":"h3","lines":"94,95"}},{"content":"Audio Understanding &amp; Reasoning","children":[{"content":"MMAU: 73.9% accuracy (Audio Thinker: 75.9%)","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"MMAR: 71.9% (+12% over baseline 60.1%)","children":[],"payload":{"tag":"li","lines":"102,103"}},{"content":"MMSU: 72.4% (+10% over baseline 62.3%)","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"MMAU-Pro: 62.9% (baseline: 59.2%)","children":[],"payload":{"tag":"li","lines":"104,106"}}],"payload":{"tag":"h3","lines":"100,101"}},{"content":"Audiovisual Benchmarks (TAC-V)","children":[{"content":"DailyOmni: 77.9% (SOTA, vs. Qwen3-Omni: 76.2%)","children":[],"payload":{"tag":"li","lines":"107,108"}},{"content":"VideoHolmes: 59.2% (vs. Qwen3-Omni: 57.3%)","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"AVHBench AVH: 81.7% (vs. PandaGPT: 58.5%)","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"AVHBench VAH: 76.6% (vs. PandaGPT: 61.3%)","children":[],"payload":{"tag":"li","lines":"110,113"}}],"payload":{"tag":"h3","lines":"106,107"}}],"payload":{"tag":"h2","lines":"80,81"}},{"content":"Key Ablations","children":[{"content":"Multitask Training","children":[{"content":"Static task settings: EvtF1 drops from 0.50 to 0.45","children":[],"payload":{"tag":"li","lines":"115,116"}},{"content":"Dynamic variation critical for generalization","children":[],"payload":{"tag":"li","lines":"116,118"}}],"payload":{"tag":"h3","lines":"114,115"}},{"content":"Pre-training Impact","children":[{"content":"Reduces hallucination from 8.8% to 4.9%","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"Clean single-source audio establishes event-caption priors","children":[],"payload":{"tag":"li","lines":"120,122"}}],"payload":{"tag":"h3","lines":"118,119"}},{"content":"LoRA Rank","children":[{"content":"Rank 128: optimal performance","children":[],"payload":{"tag":"li","lines":"123,124"}},{"content":"Rank 8: model collapse (EvtF1 plummets to 0.19)","children":[],"payload":{"tag":"li","lines":"124,126"}}],"payload":{"tag":"h3","lines":"122,123"}},{"content":"Timestamp-Weighted Loss","children":[{"content":"&#x3bb;_time = 5.0 provides best balance","children":[],"payload":{"tag":"li","lines":"127,128"}},{"content":"Without weighting, timestamps become vague and imprecise","children":[],"payload":{"tag":"li","lines":"128,130"}}],"payload":{"tag":"h3","lines":"126,127"}},{"content":"Scene Templates","children":[{"content":"Important for realistic mixture generation","children":[],"payload":{"tag":"li","lines":"131,132"}},{"content":"Improves transfer from synthetic to real-world audio","children":[],"payload":{"tag":"li","lines":"132,135"}}],"payload":{"tag":"h3","lines":"130,131"}}],"payload":{"tag":"h2","lines":"113,114"}},{"content":"Key Contributions","children":[{"content":"Describe-Then-Reason Paradigm","children":[{"content":"Decoupled perception (TAC) from reasoning (LLM)","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"Text-only reasoners achieve expert-level multimodal performance","children":[],"payload":{"tag":"li","lines":"138,139"}},{"content":"Enables test-time scaling with stronger reasoners","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"Audio understanding as a &quot;semantic bridge&quot;","children":[],"payload":{"tag":"li","lines":"140,142"}}],"payload":{"tag":"h3","lines":"136,137"}},{"content":"Dense Temporal Grounding","children":[{"content":"SOTA on TACOS dense captioning benchmark","children":[],"payload":{"tag":"li","lines":"143,144"}},{"content":"Minimal hallucination (4.9%)","children":[],"payload":{"tag":"li","lines":"144,145"}},{"content":"Controllable detail and resolution at inference time","children":[],"payload":{"tag":"li","lines":"145,147"}}],"payload":{"tag":"h3","lines":"142,143"}},{"content":"Synthetic Data Pipeline","children":[{"content":"Overcomes annotation bottleneck","children":[],"payload":{"tag":"li","lines":"148,149"}},{"content":"Infinite training data with precise temporal labels","children":[],"payload":{"tag":"li","lines":"149,150"}},{"content":"Dynamic supervision for robust generalization","children":[],"payload":{"tag":"li","lines":"150,152"}}],"payload":{"tag":"h3","lines":"147,148"}},{"content":"Audiovisual Integration","children":[{"content":"TAC-V achieves SOTA on DailyOmni and VideoHolmes","children":[],"payload":{"tag":"li","lines":"153,154"}},{"content":"Late fusion avoids cross-modal hallucination","children":[],"payload":{"tag":"li","lines":"154,155"}},{"content":"Reference-free hallucination evaluation via FLAM","children":[],"payload":{"tag":"li","lines":"155,158"}}],"payload":{"tag":"h3","lines":"152,153"}}],"payload":{"tag":"h2","lines":"135,136"}},{"content":"Limitations &amp; Future Work","children":[{"content":"Current Limitations","children":[{"content":"Simulation-to-reality gap: overestimation of dramatic events","children":[],"payload":{"tag":"li","lines":"160,161"}},{"content":"Insufficient precision for musical descriptions (chords, keys)","children":[],"payload":{"tag":"li","lines":"161,162"}},{"content":"Bias inheritance from source audio libraries","children":[],"payload":{"tag":"li","lines":"162,164"}}],"payload":{"tag":"h3","lines":"159,160"}},{"content":"Future Directions","children":[{"content":"Unsupervised domain adaptation for event prior calibration","children":[],"payload":{"tag":"li","lines":"165,166"}},{"content":"Dense multimodal conditioning for audiovisual generation","children":[],"payload":{"tag":"li","lines":"166,167"}},{"content":"TAC as semantic encoder with text latents","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"Expansion to other multimodal domains","children":[],"payload":{"tag":"li","lines":"168,169"}}],"payload":{"tag":"h3","lines":"164,165"}}],"payload":{"tag":"h2","lines":"158,159"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
