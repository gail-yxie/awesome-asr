<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Qwen3-Omni Technical Report","children":[{"content":"Problem &amp; Motivation","children":[{"content":"Modality Trade-offs","children":[{"content":"LLM-centric multimodal models often exhibit performance degradation in one modality when improved in others","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Goal: A single model that maintains state-of-the-art (SOTA) performance across text, image, audio, and video without degradation","children":[],"payload":{"tag":"li","lines":"4,6"}}],"payload":{"tag":"h3","lines":"2,3"}},{"content":"Interaction Requirements","children":[{"content":"Need for natural real-time speech and fluent text","children":[],"payload":{"tag":"li","lines":"7,8"}},{"content":"Reducing first-packet latency in streaming synthesis for high concurrency in industrial-scale deployments","children":[],"payload":{"tag":"li","lines":"8,11"}}],"payload":{"tag":"h3","lines":"6,7"}}],"payload":{"tag":"h2","lines":"1,2"}},{"content":"Architecture / Method","children":[{"content":"Thinker-Talker MoE Architecture","children":[{"content":"Thinker","children":[{"content":"Mixture-of-Experts (MoE) Transformer","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"30B total parameters, 3B active parameters (30B-A3B)","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"Handles perception and reasoning for text, image, audio, and video","children":[],"payload":{"tag":"li","lines":"16,18"}}],"payload":{"tag":"h4","lines":"13,14"}},{"content":"Talker","children":[{"content":"MoE Transformer","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"3B total parameters, 0.3B active parameters (3B-A0.3B)","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"Conditions only on audio and visual features (decoupled from Thinker text representations)","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"Generates streaming speech tokens using a multi-codebook autoregressive scheme","children":[],"payload":{"tag":"li","lines":"22,25"}}],"payload":{"tag":"h4","lines":"18,19"}}],"payload":{"tag":"h3","lines":"12,13"}},{"content":"Perception Components","children":[{"content":"Audio Transformer (AuT)","children":[{"content":"0.6B parameter attention-encoder-decoder model","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Trained on 20 million hours of supervised audio","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"Downsamples audio 8x to a token rate of 12.5 Hz (80ms segments)","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"Uses flash attention with dynamic window sizes (1 to 8 seconds)","children":[],"payload":{"tag":"li","lines":"30,32"}}],"payload":{"tag":"h4","lines":"26,27"}},{"content":"Vision Encoder","children":[{"content":"SigLIP2-So400m-based architecture","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Approximately 543 million parameters","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Handles both image and video via dynamic frame rate sampling","children":[],"payload":{"tag":"li","lines":"35,37"}}],"payload":{"tag":"h4","lines":"32,33"}},{"content":"TM-RoPE","children":[{"content":"Time-aligned Multimodal Rotary Position Embedding","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"Redistributes rotary angles: 24 temporal, 20 height, 20 width","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"Anchored to absolute time (80ms per temporal ID) to support arbitrary streaming duration","children":[],"payload":{"tag":"li","lines":"40,43"}}],"payload":{"tag":"h4","lines":"37,38"}}],"payload":{"tag":"h3","lines":"25,26"}},{"content":"Generation &amp; Streaming Modules","children":[{"content":"MTP Module","children":[{"content":"80M parameter ultra-lightweight dense Transformer","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"Predicts residual codebooks for the current frame to enhance vocal expressivity","children":[],"payload":{"tag":"li","lines":"46,48"}}],"payload":{"tag":"h4","lines":"44,45"}},{"content":"Code2Wav Renderer","children":[{"content":"200M parameter lightweight causal ConvNet","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Replaces block-wise diffusion/DiT for immediate frame-by-frame synthesis","children":[],"payload":{"tag":"li","lines":"50,54"}}],"payload":{"tag":"h4","lines":"48,49"}}],"payload":{"tag":"h3","lines":"43,44"}}],"payload":{"tag":"h2","lines":"11,12"}},{"content":"Training","children":[{"content":"Pretraining Stages","children":[{"content":"S1: Encoder Alignment","children":[{"content":"Locks LLM parameters","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Trains vision and audio adapters/encoders separately","children":[],"payload":{"tag":"li","lines":"58,60"}}],"payload":{"tag":"h4","lines":"56,57"}},{"content":"S2: General Phase","children":[{"content":"2 trillion tokens total","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"Distribution: 0.57T text, 0.77T audio, 0.82T image, 0.05T video, 0.05T video-audio","children":[],"payload":{"tag":"li","lines":"62,64"}}],"payload":{"tag":"h4","lines":"60,61"}},{"content":"S3: Long Context","children":[{"content":"Increases max token length from 8,192 to 32,768","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"Increases proportion of long audio/video data","children":[],"payload":{"tag":"li","lines":"66,69"}}],"payload":{"tag":"h4","lines":"64,65"}}],"payload":{"tag":"h3","lines":"55,56"}},{"content":"Post-training Pipelines","children":[{"content":"Thinker Training","children":[{"content":"Lightweight SFT for instruction following","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Strong-to-Weak Distillation (Alignment with Qwen3-235B)","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"GSPO (Generalized Step-level Preference Optimization) with Rule-based and Model-based (LLM-as-a-judge) rewards","children":[],"payload":{"tag":"li","lines":"73,75"}}],"payload":{"tag":"h4","lines":"70,71"}},{"content":"Talker Training","children":[{"content":"Stage 1: Mapping multimodal representations to speech","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Stage 2: Continual Pretraining (CPT) for hallucination reduction","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"Stage 3: DPO for multilingual stability","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"Stage 4: Speaker fine-tuning for naturalness and expressiveness","children":[],"payload":{"tag":"li","lines":"79,83"}}],"payload":{"tag":"h4","lines":"75,76"}}],"payload":{"tag":"h3","lines":"69,70"}}],"payload":{"tag":"h2","lines":"54,55"}},{"content":"Key Results","children":[{"content":"Audio &amp; Audiovisual Benchmarks","children":[{"content":"Open-source SOTA on 32 out of 36 benchmarks","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Overall SOTA on 22 benchmarks","children":[],"payload":{"tag":"li","lines":"86,87"}},{"content":"Outperforms Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe","children":[],"payload":{"tag":"li","lines":"87,89"}}],"payload":{"tag":"h3","lines":"84,85"}},{"content":"Specific Metrics","children":[{"content":"ASR (WER)","children":[{"content":"Librispeech Clean/Other: 1.22 / 2.48","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"Wenetspeech Meeting: 4.69 / 5.89","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"Fleurs-en: 2.72","children":[],"payload":{"tag":"li","lines":"93,95"}}],"payload":{"tag":"h4","lines":"90,91"}},{"content":"Latency","children":[{"content":"Theoretical end-to-end first-packet latency: 234 ms (Audio), 547 ms (Video)","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Generation Real Time Factor (RTF) remains &lt; 1 under high concurrency","children":[],"payload":{"tag":"li","lines":"97,99"}}],"payload":{"tag":"h4","lines":"95,96"}},{"content":"Music Understanding","children":[{"content":"RUL-MuchoMusic: 52.0 (New SOTA)","children":[],"payload":{"tag":"li","lines":"100,101"}},{"content":"GTZAN Accuracy: 93.0","children":[],"payload":{"tag":"li","lines":"101,103"}}],"payload":{"tag":"h4","lines":"99,100"}},{"content":"Text Reasoning","children":[{"content":"GPQA: 73.1 (Thinking model)","children":[],"payload":{"tag":"li","lines":"104,105"}},{"content":"AIME25: 73.7 (Thinking model)","children":[],"payload":{"tag":"li","lines":"105,106"}},{"content":"ZebraLogic: 76.1 (Instruct model)","children":[],"payload":{"tag":"li","lines":"106,109"}}],"payload":{"tag":"h4","lines":"103,104"}}],"payload":{"tag":"h3","lines":"89,90"}},{"content":"Zero-Shot TTS","children":[{"content":"SEED test-en WER: 1.39","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Competitive with dedicated systems like F5-TTS and CosyVoice 3","children":[],"payload":{"tag":"li","lines":"111,114"}}],"payload":{"tag":"h3","lines":"109,110"}}],"payload":{"tag":"h2","lines":"83,84"}},{"content":"Contributions","children":[{"content":"Model Parity","children":[],"payload":{"tag":"h3","lines":"115,116"}},{"content":"AuT Encoder","children":[],"payload":{"tag":"h3","lines":"117,118"}},{"content":"Thinking Model","children":[],"payload":{"tag":"h3","lines":"119,120"}},{"content":"Streaming Innovation","children":[],"payload":{"tag":"h3","lines":"121,122"}},{"content":"Audio Captioner","children":[],"payload":{"tag":"h3","lines":"123,124"}}],"payload":{"tag":"h2","lines":"114,115"}},{"content":"Available Resources","children":[{"content":"Open Source Release","children":[{"content":"Qwen3-Omni-30B-A3B (Base/Instruct)","children":[],"payload":{"tag":"li","lines":"128,129"}},{"content":"Qwen3-Omni-30B-A3B-Thinking","children":[],"payload":{"tag":"li","lines":"129,130"}},{"content":"Qwen3-Omni-30B-A3B-Captioner","children":[],"payload":{"tag":"li","lines":"130,132"}}],"payload":{"tag":"h3","lines":"127,128"}},{"content":"License","children":[],"payload":{"tag":"h3","lines":"132,133"}},{"content":"Capabilities","children":[{"content":"Text interaction: 119 languages","children":[],"payload":{"tag":"li","lines":"135,136"}},{"content":"Speech understanding: 19 languages","children":[],"payload":{"tag":"li","lines":"136,137"}},{"content":"Speech generation: 10 languages","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"Long audio support: Up to 40 minutes per instance","children":[],"payload":{"tag":"li","lines":"138,141"}}],"payload":{"tag":"h3","lines":"134,135"}}],"payload":{"tag":"h2","lines":"126,127"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
