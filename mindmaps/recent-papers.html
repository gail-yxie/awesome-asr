<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Recent ASR &amp; Speech Language Papers","children":[{"content":"cs.CL","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.23300v1\">A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations</a>","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"<a href=\"https://arxiv.org/abs/2602.22522v1\">Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing</a>","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21741v1\">Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization</a>","children":[],"payload":{"tag":"li","lines":"5,6"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21647v1\">Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration</a>","children":[],"payload":{"tag":"li","lines":"6,7"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21646v1\">Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion</a>","children":[],"payload":{"tag":"li","lines":"7,8"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21741v1\">Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization</a>","children":[],"payload":{"tag":"li","lines":"8,9"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21647v1\">Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration</a>","children":[],"payload":{"tag":"li","lines":"9,10"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21646v1\">Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion</a>","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"<a href=\"https://arxiv.org/abs/2602.19991v1\">Cross-lingual Matryoshka Representation Learning across Speech and Text</a>","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"<a href=\"https://arxiv.org/abs/2602.15537v1\">ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling</a>","children":[],"payload":{"tag":"li","lines":"12,14"}}],"payload":{"tag":"h2","lines":"2,3"}},{"content":"cs.NI","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.17394v1\">Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks</a>","children":[],"payload":{"tag":"li","lines":"15,17"}}],"payload":{"tag":"h2","lines":"14,15"}},{"content":"cs.SD","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.23070v1\">Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment</a>","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"<a href=\"https://arxiv.org/abs/2602.23068v1\">TADA: A Generative Framework for Speech Modeling via Text-Acoustic Dual Alignment</a>","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"<a href=\"https://arxiv.org/abs/2602.22935v1\">A Holistic Framework for Robust Bangla ASR and Speaker Diarization with Optimized VAD and CTC Alignment</a>","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<a href=\"https://arxiv.org/abs/2602.21183v1\">823-OLT @ BUET DL Sprint 4.0: Context-Aware Windowing for ASR and Fine-Tuned Speaker Diarization in Bengali Long Form Audio</a>","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<a href=\"https://arxiv.org/abs/2602.05406\">Enabling Automatic Disordered Speech Recognition: An Impaired Speech Dataset in the Akan Language</a>","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"h2","lines":"17,18"}},{"content":"eess.AS","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.22039v1\">TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition</a>","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"<a href=\"https://arxiv.org/abs/2602.20967v1\">Training-Free Intelligibility-Guided Observation Addition for Noisy ASR</a>","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"<a href=\"https://arxiv.org/abs/2602.22039v1\">TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition</a>","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"<a href=\"https://arxiv.org/abs/2602.19574v1\">CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment</a>","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"<a href=\"https://arxiv.org/abs/2602.14584v1\">CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia</a>","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"<a href=\"https://arxiv.org/abs/2602.01008\">Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages</a>","children":[],"payload":{"tag":"li","lines":"30,31"}}],"payload":{"tag":"h2","lines":"24,25"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
