<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Recent ASR &amp; Speech Language Papers","children":[{"content":"cs.CL","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.19991v1\">Cross-lingual Matryoshka Representation Learning across Speech and Text</a>","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"<a href=\"https://arxiv.org/abs/2602.15537v1\">ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling</a>","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"<a href=\"https://arxiv.org/abs/2602.14062\">From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset</a>","children":[],"payload":{"tag":"li","lines":"5,7"}}],"payload":{"tag":"h2","lines":"2,3"}},{"content":"cs.NI","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.17394v1\">Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks</a>","children":[],"payload":{"tag":"li","lines":"8,10"}}],"payload":{"tag":"h2","lines":"7,8"}},{"content":"cs.SD","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.05406\">Enabling Automatic Disordered Speech Recognition: An Impaired Speech Dataset in the Akan Language</a>","children":[],"payload":{"tag":"li","lines":"11,13"}}],"payload":{"tag":"h2","lines":"10,11"}},{"content":"eess.AS","children":[{"content":"<a href=\"https://arxiv.org/abs/2602.19574v1\">CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment</a>","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"<a href=\"https://arxiv.org/abs/2602.14584v1\">CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia</a>","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"<a href=\"https://arxiv.org/abs/2602.01008\">Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages</a>","children":[],"payload":{"tag":"li","lines":"16,17"}}],"payload":{"tag":"h2","lines":"13,14"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
