<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Qwen3-ASR Technical Report","children":[{"content":"Problem &amp; Motivation","children":[{"content":"Transition from traditional E2E (Transducer/AED) to Large Audio-Language Model (LALM) paradigm","children":[],"payload":{"tag":"li","lines":"2,3"}},{"content":"Leveraging LLM world knowledge and reasoning for challenging scenarios (long-form, noise, named entities)","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Need for robust multilingual and dialectal ASR (standard models often hit annotation error limits)","children":[],"payload":{"tag":"li","lines":"4,5"}},{"content":"Requirement for high-accuracy timestamps in ASR outputs (e.g., subtitles)","children":[],"payload":{"tag":"li","lines":"5,6"}},{"content":"Lack of a unified, multilingual, and efficient forced alignment system","children":[],"payload":{"tag":"li","lines":"6,8"}}],"payload":{"tag":"h2","lines":"1,2"}},{"content":"Architecture / Method","children":[{"content":"Qwen3-ASR Family","children":[{"content":"Models: Qwen3-ASR-1.7B (300M encoder + 1.7B LLM) and Qwen3-ASR-0.6B (180M encoder + 0.6B LLM)","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"Foundation: Built on Qwen3-Omni with strong audio understanding","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"AuT Encoder: AED-based, 8x downsampling, 12.5Hz token rate, 128-dim Fbank features","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"Dynamic Flash Attention: Window size 1s to 8s supporting both streaming and offline inference","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"Projector: Connects AuT encoder embeddings to the Qwen3 LLM backbone","children":[],"payload":{"tag":"li","lines":"14,16"}}],"payload":{"tag":"h3","lines":"9,10"}},{"content":"Qwen3-ForcedAligner-0.6B","children":[{"content":"Formulation: Reframes forced alignment as a Non-Autoregressive (NAR) slot-filling task","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Special Tokens: Uses [time] tokens inserted into transcripts as timestamp slots","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Prediction: A linear layer predicts discrete timestamp indices (80ms frame duration)","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Capacity: Supports up to 300s audio (3,750 classes) with 11 languages","children":[],"payload":{"tag":"li","lines":"20,23"}}],"payload":{"tag":"h3","lines":"16,17"}}],"payload":{"tag":"h2","lines":"8,9"}},{"content":"Training","children":[{"content":"ASR Training Pipeline","children":[{"content":"Stage 1: AuT Pretraining - 40 million hours of pseudo-labeled data (primarily English/Chinese)","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Stage 2: Omni Pretraining - 3 trillion tokens across audio, vision, and text for multimodal capability","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"Stage 3: ASR SFT - Style transfer on input/output formats, includes context biasing and streaming-enhancement data","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Stage 4: ASR RL - Group Sequence Policy Optimization (GSPO) using 50k utterances for noise robustness and stability","children":[],"payload":{"tag":"li","lines":"28,30"}}],"payload":{"tag":"h3","lines":"24,25"}},{"content":"ForcedAligner Training","children":[{"content":"Data: Distillation and smoothing of pseudo-labels from Montreal Forced Aligner (MFA)","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Loss: Cross-entropy loss applied specifically to timestamp slots","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Strategy: Causal training with non-shifted sequences; dynamic slot insertion for generalization","children":[],"payload":{"tag":"li","lines":"33,36"}}],"payload":{"tag":"h3","lines":"30,31"}}],"payload":{"tag":"h2","lines":"23,24"}},{"content":"Key Results","children":[{"content":"ASR Accuracy","children":[{"content":"Qwen3-ASR-1.7B: SOTA among open-source models; competitive with GPT-4o and Gemini-2.5-Pro","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"WenetSpeech (Mandarin): 4.97 CER (Meeting subset) for 1.7B model","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"GigaSpeech (English): 8.45 WER for 1.7B model","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Dialects: Supports 22 Chinese dialects; 15.94 average CER on internal dialect suite","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"h3","lines":"37,38"}},{"content":"Efficiency (0.6B Model)","children":[{"content":"TTFT: Average as low as 92ms","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"RTF: 0.064 at concurrency 128","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"Throughput: Processes 2,000 seconds of audio per 1 second of wall time","children":[],"payload":{"tag":"li","lines":"46,48"}}],"payload":{"tag":"h3","lines":"43,44"}},{"content":"Forced Alignment","children":[{"content":"Accuracy: 67% to 77% reduction in accumulated average shift (AAS) vs MFA/NFA/WhisperX","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"AAS: 32.4ms on human-labeled test sets (vs 101.2ms for NFA)","children":[],"payload":{"tag":"li","lines":"50,52"}}],"payload":{"tag":"h3","lines":"48,49"}},{"content":"Specialized Tasks","children":[{"content":"Singing Voice: Strong performance on M4Singer and Opencpop","children":[],"payload":{"tag":"li","lines":"53,54"}},{"content":"Song Transcription: Robust to background music (BGM) in long-form songs","children":[],"payload":{"tag":"li","lines":"54,57"}}],"payload":{"tag":"h3","lines":"52,53"}}],"payload":{"tag":"h2","lines":"36,37"}},{"content":"Contributions","children":[{"content":"Novel Architecture","children":[],"payload":{"tag":"h3","lines":"58,59"}},{"content":"Language Coverage","children":[],"payload":{"tag":"h3","lines":"60,61"}},{"content":"Unified Inference","children":[],"payload":{"tag":"h3","lines":"62,63"}},{"content":"Robustness","children":[],"payload":{"tag":"h3","lines":"64,65"}},{"content":"Alignment Versatility","children":[],"payload":{"tag":"h3","lines":"66,67"}}],"payload":{"tag":"h2","lines":"57,58"}},{"content":"Available Resources","children":[{"content":"Models","children":[{"content":"Qwen3-ASR-1.7B (HuggingFace)","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Qwen3-ASR-0.6B (HuggingFace)","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"Qwen3-ForcedAligner-0.6B","children":[],"payload":{"tag":"li","lines":"73,75"}}],"payload":{"tag":"h3","lines":"70,71"}},{"content":"Code &amp; Tools","children":[{"content":"GitHub: https://github.com/QwenLM/Qwen3-ASR","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Inference framework: vLLM-based (ASR) and Transformers-based (FA)","children":[],"payload":{"tag":"li","lines":"77,79"}}],"payload":{"tag":"h3","lines":"75,76"}},{"content":"License","children":[],"payload":{"tag":"h3","lines":"79,80"}}],"payload":{"tag":"h2","lines":"69,70"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
