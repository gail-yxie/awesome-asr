<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Qwen3-ASR Technical Report","children":[{"content":"Problem &amp; Motivation","children":[{"content":"Paradigm Shift","children":[{"content":"Transition from traditional E2E (Transducer/AED) to Large Audio-Language Model (LALM) paradigm","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Need to leverage LLM world knowledge and language modeling for ASR robustness","children":[],"payload":{"tag":"li","lines":"4,6"}}],"payload":{"tag":"h3","lines":"2,3"}},{"content":"Current Limitations","children":[{"content":"Inconsistencies in ASR quality in real-world scenarios vs open benchmarks","children":[],"payload":{"tag":"li","lines":"7,8"}},{"content":"Lack of unified, multilingual forced alignment systems for LALMs","children":[],"payload":{"tag":"li","lines":"8,9"}},{"content":"Difficulty in balancing high accuracy with low latency for on-device deployment","children":[],"payload":{"tag":"li","lines":"9,11"}}],"payload":{"tag":"h3","lines":"6,7"}},{"content":"Functional Gaps","children":[{"content":"Requirement for accurate word-level/sentence-level timestamps for subtitles","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"Need for robustness to noise, accents, dialects, and singing voice","children":[],"payload":{"tag":"li","lines":"13,16"}}],"payload":{"tag":"h3","lines":"11,12"}}],"payload":{"tag":"h2","lines":"1,2"}},{"content":"Architecture / Method","children":[{"content":"Model Family","children":[{"content":"Qwen3-ASR-1.7B: SOTA open-source performance, competitive with proprietary APIs","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Qwen3-ASR-0.6B: Optimized for accuracy-efficiency trade-off and on-device deployment","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Qwen3-ForcedAligner-0.6B: LLM-based Non-Autoregressive (NAR) timestamp predictor","children":[],"payload":{"tag":"li","lines":"20,22"}}],"payload":{"tag":"h3","lines":"17,18"}},{"content":"Foundation Model","children":[{"content":"Post-trained from Qwen3-Omni foundation model","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Inherits multi-modal understanding capabilities","children":[],"payload":{"tag":"li","lines":"24,26"}}],"payload":{"tag":"h3","lines":"22,23"}},{"content":"Audio Encoder (AuT)","children":[{"content":"AED-based architecture with 8x downsampling","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"Fbank features: 128 dimensions","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"Token rate: 12.5Hz (80ms frame duration)","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"Dynamic Flash Attention: Window sizes 1s to 8s for unified streaming/offline inference","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"Parameters: 300M (for 1.7B model, 1024 hidden size) and 180M (for 0.6B model, 896 hidden size)","children":[],"payload":{"tag":"li","lines":"31,33"}}],"payload":{"tag":"h3","lines":"26,27"}},{"content":"Qwen3-ForcedAligner-0.6B Architecture","children":[{"content":"Reframes forced alignment as a slot-filling task","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Uses [time] special tokens as word/character boundary slots","children":[],"payload":{"tag":"li","lines":"35,36"}},{"content":"Non-autoregressive (NAR) decoding for simultaneous timestamp prediction","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"Supports speech inputs up to 300 seconds","children":[],"payload":{"tag":"li","lines":"37,40"}}],"payload":{"tag":"h3","lines":"33,34"}}],"payload":{"tag":"h2","lines":"16,17"}},{"content":"Training","children":[{"content":"Four-Stage Pipeline","children":[{"content":"1. AuT Pretraining","children":[{"content":"AED framework with ~40 million hours of pseudo-labeled ASR data","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"Focus on Chinese and English for stable audio representations","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"h4","lines":"42,43"}},{"content":"2. Omni Pretraining","children":[{"content":"Trained on 3 trillion tokens across audio, vision, and text tasks","children":[],"payload":{"tag":"li","lines":"47,49"}}],"payload":{"tag":"h4","lines":"46,47"}},{"content":"3. ASR Supervised Finetuning (SFT)","children":[{"content":"Style transfer for input/output formatting","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Includes non-speech data, streaming-enhancement data, and context biasing","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Mitigates instruction injection by making it ASR-only (no instruction following)","children":[],"payload":{"tag":"li","lines":"52,54"}}],"payload":{"tag":"h4","lines":"49,50"}},{"content":"4. ASR Reinforcement Learning (RL)","children":[{"content":"Utilizes Group Sequence Policy Optimization (GSPO)","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"Data: 50k utterances (35% CN/EN, 35% Multilingual, 30% Functional)","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Improves noise robustness and transcription stability","children":[],"payload":{"tag":"li","lines":"57,60"}}],"payload":{"tag":"h4","lines":"54,55"}}],"payload":{"tag":"h3","lines":"41,42"}},{"content":"ForcedAligner Training","children":[{"content":"Distilled and smoothed from Montreal Forced Aligner (MFA) pseudo-labels","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"Causal training strategy to ensure global consistency without position offsets","children":[],"payload":{"tag":"li","lines":"62,63"}},{"content":"Dynamic slot insertion strategy to improve generalization","children":[],"payload":{"tag":"li","lines":"63,66"}}],"payload":{"tag":"h3","lines":"60,61"}}],"payload":{"tag":"h2","lines":"40,41"}},{"content":"Key Results","children":[{"content":"ASR Performance","children":[{"content":"Supports 30 languages and 22 Chinese dialects","children":[],"payload":{"tag":"li","lines":"68,69"}},{"content":"SOTA on WenetSpeech (4.97-5.88 CER) and LibriSpeech (1.63-3.38 WER)","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"Outperforms Whisper-large-v3 and competitive with GPT-4o-Transcribe on internal robustness suites","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"Robust to singing voice (M4Singer, Popcs) and full songs with BGM","children":[],"payload":{"tag":"li","lines":"71,73"}}],"payload":{"tag":"h3","lines":"67,68"}},{"content":"Inference Efficiency (0.6B Model)","children":[{"content":"Average TTFT (Time-to-First-Token): 92ms","children":[],"payload":{"tag":"li","lines":"74,75"}},{"content":"Real-Time Factor (RTF): 0.064","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"Throughput: 2,000 seconds of audio per second at concurrency 128","children":[],"payload":{"tag":"li","lines":"76,78"}}],"payload":{"tag":"h3","lines":"73,74"}},{"content":"Language Identification (LID)","children":[{"content":"Average accuracy of 97.9% for 1.7B model across 4 benchmarks","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"Stable performance on 30 languages including long-tail distributions","children":[],"payload":{"tag":"li","lines":"80,82"}}],"payload":{"tag":"h3","lines":"78,79"}},{"content":"Forced Alignment Accuracy","children":[{"content":"Accumulated Average Shift (AAS) reduced by 67%~77% compared to MFA/NFA/WhisperX","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"Maintains high accuracy on long utterances (300s) where baselines often degrade","children":[],"payload":{"tag":"li","lines":"84,87"}}],"payload":{"tag":"h3","lines":"82,83"}}],"payload":{"tag":"h2","lines":"66,67"}},{"content":"Contributions","children":[{"content":"Unified Modeling","children":[{"content":"First Large Language Model based speech forced aligner for flexible granularities","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"Supports offline and streaming inference in a single model","children":[],"payload":{"tag":"li","lines":"90,92"}}],"payload":{"tag":"h3","lines":"88,89"}},{"content":"Capability Expansion","children":[{"content":"Robust support for 52 languages and dialects","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"Advanced singing-voice recognition and robustness to background music (BGM)","children":[],"payload":{"tag":"li","lines":"94,96"}}],"payload":{"tag":"h3","lines":"92,93"}},{"content":"Open Science","children":[{"content":"Released model weights under Apache 2.0 license","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Open-source codebase for inference and fine-tuning recipes","children":[],"payload":{"tag":"li","lines":"98,101"}}],"payload":{"tag":"h3","lines":"96,97"}}],"payload":{"tag":"h2","lines":"87,88"}},{"content":"Available Resources","children":[{"content":"Models","children":[{"content":"Qwen/Qwen3-ASR-1.7B (HuggingFace)","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"Qwen/Qwen3-ASR-0.6B (HuggingFace)","children":[],"payload":{"tag":"li","lines":"104,105"}},{"content":"Qwen/Qwen3-ForcedAligner-0.6B (HuggingFace)","children":[],"payload":{"tag":"li","lines":"105,107"}}],"payload":{"tag":"h3","lines":"102,103"}},{"content":"Code","children":[{"content":"GitHub: QwenLM/Qwen3-ASR","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"Unified toolkit for multi-granularity alignment, streaming, and SFT","children":[],"payload":{"tag":"li","lines":"109,112"}}],"payload":{"tag":"h3","lines":"107,108"}}],"payload":{"tag":"h2","lines":"101,102"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
