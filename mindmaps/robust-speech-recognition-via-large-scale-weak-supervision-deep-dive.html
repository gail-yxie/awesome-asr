<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)","children":[{"content":"Problem &amp; Motivation","children":[{"content":"Generalization Gap","children":[{"content":"Traditional supervised ASR models overfit to specific datasets (e.g., LibriSpeech).","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"Performance drops significantly when applied to out-of-distribution (OOD) data (noise, accents, technical jargon).","children":[],"payload":{"tag":"li","lines":"4,6"}}],"payload":{"tag":"h3","lines":"2,3"}},{"content":"Pre-training vs. Zero-shot","children":[{"content":"Self-supervised models (e.g., wav2vec 2.0) require manual fine-tuning on clean data.","children":[],"payload":{"tag":"li","lines":"7,8"}},{"content":"Goal: Create a model that works &apos;out of the box&apos; across diverse domains without fine-tuning.","children":[],"payload":{"tag":"li","lines":"8,10"}}],"payload":{"tag":"h3","lines":"6,7"}},{"content":"Robustness","children":[{"content":"Existing systems are brittle to recording conditions.","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"Human speech recognition is highly robust; ASR should match this capability.","children":[],"payload":{"tag":"li","lines":"12,15"}}],"payload":{"tag":"h3","lines":"10,11"}}],"payload":{"tag":"h2","lines":"1,2"}},{"content":"Architecture / Method","children":[{"content":"Frontend","children":[{"content":"Audio sampled at 16,000 Hz.","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Input: 80-channel log-Magnitude Mel Spectrogram.","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Window size: 25ms, Stride: 10ms.","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"Input segmented into 30-second fixed-length chunks (padded or truncated).","children":[],"payload":{"tag":"li","lines":"20,22"}}],"payload":{"tag":"h3","lines":"16,17"}},{"content":"Encoder","children":[{"content":"Transformer-based architecture.","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Two 1D convolutional layers with filter size 3 and stride 2 (subsampling).","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"Sinusoidal Positional Embeddings.","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Stack of Transformer blocks (Self-attention + MLP).","children":[],"payload":{"tag":"li","lines":"26,28"}}],"payload":{"tag":"h3","lines":"22,23"}},{"content":"Decoder","children":[{"content":"Transformer-based decoder.","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"Cross-attention over encoder hidden states.","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"Sinusoidal Positional Embeddings.","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Learned task-specific prompting via special tokens.","children":[],"payload":{"tag":"li","lines":"32,34"}}],"payload":{"tag":"h3","lines":"28,29"}},{"content":"Multitask Format","children":[{"content":"Special Tokens","children":[{"content":"&lt;|startoftranscript|&gt;: Signals beginning of prediction.","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"&lt;|en|&gt;, &lt;|fr|&gt;, etc.: Language Identification (LID) tokens.","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"&lt;|transcribe|&gt;: Task token for ASR.","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"&lt;|translate|&gt;: Task token for Speech-to-English translation.","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"&lt;|notimestamps|&gt;: Toggle for timestamp prediction.","children":[],"payload":{"tag":"li","lines":"40,42"}}],"payload":{"tag":"h4","lines":"35,36"}},{"content":"Timestamps","children":[{"content":"Predicts relative timestamps within the 30s window.","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"Quantized at 20ms resolution.","children":[],"payload":{"tag":"li","lines":"44,48"}}],"payload":{"tag":"h4","lines":"42,43"}}],"payload":{"tag":"h3","lines":"34,35"}}],"payload":{"tag":"h2","lines":"15,16"}},{"content":"Training","children":[{"content":"Dataset","children":[{"content":"Total: 680,000 hours of multilingual/multitask audio.","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"438,000 hours of English speech + transcripts.","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"126,000 hours of non-English speech + transcripts.","children":[],"payload":{"tag":"li","lines":"52,53"}},{"content":"117,000 hours of X-to-English translation data.","children":[],"payload":{"tag":"li","lines":"53,55"}}],"payload":{"tag":"h3","lines":"49,50"}},{"content":"Data Processing","children":[{"content":"Weak supervision: Transcripts collected from the internet.","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Automated filtering: Removed machine-generated captions and low-quality transcriptions using heuristics (e.g., lack of punctuation, uppercase-only).","children":[],"payload":{"tag":"li","lines":"57,59"}}],"payload":{"tag":"h3","lines":"55,56"}},{"content":"Procedure","children":[{"content":"Loss: Cross-entropy (standard sequence-to-sequence objective).","children":[],"payload":{"tag":"li","lines":"60,61"}},{"content":"Optimizer: AdamW with linear learning rate decay.","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"Batch size: 256 segments.","children":[],"payload":{"tag":"li","lines":"62,63"}},{"content":"Trained across multiple model sizes for scaling analysis.","children":[],"payload":{"tag":"li","lines":"63,66"}}],"payload":{"tag":"h3","lines":"59,60"}}],"payload":{"tag":"h2","lines":"48,49"}},{"content":"Key Results","children":[{"content":"Zero-shot Performance","children":[{"content":"Whisper matches or exceeds fully supervised models on many datasets without seeing a single training sample from them.","children":[],"payload":{"tag":"li","lines":"68,69"}},{"content":"Competitive on LibriSpeech (though not SOTA) but significantly more robust on OOD benchmarks like Fleurs or Common Voice.","children":[],"payload":{"tag":"li","lines":"69,71"}}],"payload":{"tag":"h3","lines":"67,68"}},{"content":"Robustness Comparison","children":[{"content":"At the same LibriSpeech WER, Whisper is significantly more robust to background noise than Wav2vec 2.0.","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"Approaches human-level robustness and accuracy on several benchmarks.","children":[],"payload":{"tag":"li","lines":"73,75"}}],"payload":{"tag":"h3","lines":"71,72"}},{"content":"Scaling Laws","children":[{"content":"Error rate decreases predictably as model capacity and dataset size increase.","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Multilingual performance improves English ASR due to cross-lingual transfer.","children":[],"payload":{"tag":"li","lines":"77,79"}}],"payload":{"tag":"h3","lines":"75,76"}},{"content":"Language Support","children":[{"content":"Strong performance on high-resource languages.","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"Translation capability (X-to-English) outperforms many dedicated supervised translation models.","children":[],"payload":{"tag":"li","lines":"81,84"}}],"payload":{"tag":"h3","lines":"79,80"}}],"payload":{"tag":"h2","lines":"66,67"}},{"content":"Contributions","children":[{"content":"Large-scale Weak Supervision","children":[],"payload":{"tag":"h3","lines":"85,86"}},{"content":"Unified Multitask Architecture","children":[],"payload":{"tag":"h3","lines":"87,88"}},{"content":"Zero-shot Generalization","children":[],"payload":{"tag":"h3","lines":"89,90"}},{"content":"Robustness Benchmark","children":[],"payload":{"tag":"h3","lines":"91,92"}}],"payload":{"tag":"h2","lines":"84,85"}},{"content":"Available Resources","children":[{"content":"Models","children":[{"content":"Tiny: 39M parameters","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Base: 74M parameters","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Small: 244M parameters","children":[],"payload":{"tag":"li","lines":"98,99"}},{"content":"Medium: 769M parameters","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"Large: 1550M parameters","children":[],"payload":{"tag":"li","lines":"100,102"}}],"payload":{"tag":"h3","lines":"95,96"}},{"content":"Software","children":[{"content":"Official Python implementation available via &apos;openai/whisper&apos; GitHub repository.","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"Inference code and pre-trained weights released under MIT license.","children":[],"payload":{"tag":"li","lines":"104,107"}}],"payload":{"tag":"h3","lines":"102,103"}}],"payload":{"tag":"h2","lines":"94,95"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
