{
  "date": "2026-02-19",
  "summary": "Today's research highlights a push toward more efficient Spoken Language Modeling (SLM) with the introduction of **ZeroSyl**, a zero-resource syllable tokenization method designed to mitigate the sequence length explosion typical of discrete units from self-supervised encoders. This work addresses a critical bottleneck in \"textless\" NLP by demonstrating that syllable-like units can be derived without textual supervision to improve the modeling efficiency and performance of raw audio-based models. Complementing this, new model releases include several language-specific fine-tunes of **Metaâ€™s MMS-1b** (covering codes such as aft, meh, and koo) and a **Whisper-large-v2** fine-tune for Welsh, continuing the trend of adapting massive foundation models to low-resource and regional languages. Together, these developments reflect a dual focus on architectural efficiency for speech-only models and the practical expansion of multilingual ASR coverage through targeted fine-tuning.",
  "ideas": [
    "ZeroSyl introduces a simplified, zero-resource approach to grouping discrete speech units into syllable-like tokens to address the high sequence length of self-supervised representations.",
    "The method effectively reduces the number of tokens required to represent speech, allowing spoken language models to process longer-range dependencies more efficiently.",
    "Unlike existing techniques that require complex training or textual resources, ZeroSyl relies on a straightforward heuristic to define segment boundaries without any supervision.",
    "The research demonstrates that syllable-level tokenization enhances the performance of pure speech language models on downstream tasks while decreasing computational costs.",
    "This approach highlights the potential for linguistically motivated units to be derived directly from the acoustic characteristics of self-supervised speech representations."
  ],
  "papers": [
    {
      "id": "2602.15537v1",
      "title": "ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling",
      "authors": [
        "Nicol Visser",
        "Simon Malan",
        "Danel Slabbert",
        "Herman Kamper"
      ],
      "abstract": "Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15537v1",
      "pdf_url": "https://arxiv.org/pdf/2602.15537v1"
    }
  ],
  "models": [
    {
      "model_id": "vitthalbhandari/mms-1b-all-aft-all-meh",
      "author": "vitthalbhandari",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/vitthalbhandari/mms-1b-all-aft-all-meh",
      "created_at": "2026-02-19",
      "pipeline_tag": "automatic-speech-recognition"
    },
    {
      "model_id": "vitthalbhandari/mms-1b-all-aft-all-lke",
      "author": "vitthalbhandari",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/vitthalbhandari/mms-1b-all-aft-all-lke",
      "created_at": "2026-02-19",
      "pipeline_tag": "automatic-speech-recognition"
    },
    {
      "model_id": "kzmaker/faster-whisper-tilsync-34000",
      "author": "kzmaker",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/kzmaker/faster-whisper-tilsync-34000",
      "created_at": "2026-02-19",
      "pipeline_tag": "automatic-speech-recognition"
    },
    {
      "model_id": "vitthalbhandari/mms-1b-all-aft-all-led",
      "author": "vitthalbhandari",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/vitthalbhandari/mms-1b-all-aft-all-led",
      "created_at": "2026-02-19",
      "pipeline_tag": "automatic-speech-recognition"
    },
    {
      "model_id": "vitthalbhandari/mms-1b-all-aft-all-koo",
      "author": "vitthalbhandari",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/vitthalbhandari/mms-1b-all-aft-all-koo",
      "created_at": "2026-02-19",
      "pipeline_tag": "automatic-speech-recognition"
    },
    {
      "model_id": "DewiBrynJones/whisper-large-v2-ft-cy-2602",
      "author": "DewiBrynJones",
      "downloads": 0,
      "likes": 0,
      "url": "https://huggingface.co/DewiBrynJones/whisper-large-v2-ft-cy-2602",
      "created_at": "2026-02-18",
      "pipeline_tag": "automatic-speech-recognition"
    }
  ],
  "datasets": [],
  "tweets": [],
  "stats": {
    "paper_count": 1,
    "model_count": 6,
    "dataset_count": 0,
    "tweet_count": 0
  }
}