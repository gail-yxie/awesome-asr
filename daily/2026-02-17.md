# ASR Daily Update â€” 2026-02-17

## Summary

Today saw several notable developments in ASR research. A new paper proposes a streaming Conformer variant that achieves sub-100ms latency while maintaining competitive WER on LibriSpeech. Meanwhile, HuggingFace saw the release of a fine-tuned Whisper model optimized for medical transcription, and a new multilingual dataset covering 15 low-resource African languages was published.

## New Papers (3)

| Title | Authors | Category | Link |
|-------|---------|----------|------|
| StreamConformer: Ultra-Low Latency Streaming ASR with Chunked Attention | Alice Chen, Bob Zhang, Carol Liu et al. | cs.CL | [arXiv](https://arxiv.org/abs/2602.12345) |
| WhisperMed: Domain-Adapted Speech Recognition for Clinical Documentation | Eva Martinez, Frank Wu | cs.CL | [arXiv](https://arxiv.org/abs/2602.12346) |
| AfriSpeech-15: A Multilingual ASR Dataset for 15 African Languages | Grace Okafor, Henry Mensah, Irene Adeyemi | cs.CL | [arXiv](https://arxiv.org/abs/2602.12347) |

## New Models & Datasets (3)

| Name | Author | Type | Link |
|------|--------|------|------|
| medai/whisper-large-v3-medical | medai | model | [HF](https://huggingface.co/medai/whisper-large-v3-medical) |
| streaming-asr/stream-conformer-base | streaming-asr | model | [HF](https://huggingface.co/streaming-asr/stream-conformer-base) |
| afrispeech/afrispeech-15 | afrispeech | dataset | [HF](https://huggingface.co/datasets/afrispeech/afrispeech-15) |

## Key Ideas

- Streaming ASR can now achieve sub-100ms latency without significant quality degradation using chunked attention patterns
- Domain-specific fine-tuning of Whisper continues to show strong results, particularly for medical terminology
- The gap in ASR performance for low-resource African languages is being actively addressed with new curated datasets
- Self-supervised pre-training on unlabeled speech data remains the most effective approach for low-resource settings
