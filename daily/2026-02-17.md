# ASR Daily Update — 2026-02-17

## Summary

A strong day for ASR research. Mistral AI releases Voxtral Realtime, a natively streaming ASR model matching offline transcription quality at sub-500ms latency across 13 languages. A decoder-only Conformer with modality-aware sparse MoE achieves 2.8% WER on LibriSpeech test-clean with only 113M parameters. Research on very long context ASR shows optimal performance at ~21.8 minutes of context, and URSA-GAN tackles cross-domain speech adaptation with 16% relative improvement.

## New Papers (6)

| Title | Authors | Category | Link |
|-------|---------|----------|------|
| Voxtral Realtime | Alexander H. Liu, Andy Ehrenberg, Andy Lo et al. | cs.AI | [arXiv](https://arxiv.org/abs/2602.11298) |
| Decoder-only Conformer with Modality-aware Sparse Mixtures of Experts for ASR | Jaeyoung Lee, Masato Mimura | eess.AS | [arXiv](https://arxiv.org/abs/2602.12546) |
| Beyond the Utterance: An Empirical Study of Very Long Context Speech Recognition | Robert Flynn, Anton Ragni | eess.AS | [arXiv](https://arxiv.org/abs/2602.09044) |
| Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement | Chien-Chun Wang, Hung-Shin Lee, Hsin-Min Wang et al. | eess.AS | [arXiv](https://arxiv.org/abs/2602.04307) |
| Where Are We At with Automatic Speech Recognition for the Bambara Language? | Seydou Diallo, Yacouba Diarra, Mamadou K. Keita et al. | cs.CL | [arXiv](https://arxiv.org/abs/2602.09785) |
| When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding | Domenico De Cristofaro, Alessandro Vietti, Marianne Pouplier et al. | cs.CL | [arXiv](https://arxiv.org/abs/2602.10350) |

## New Models & Datasets (4)

| Name | Architecture | Key Innovation | Link |
|------|-------------|----------------|------|
| openai/whisper-large-v3-turbo | Encoder-decoder Transformer, pruned decoder (809M, 4 layers) | Decoder pruned from 32 to 4 layers for 5-8x faster inference with minimal accuracy loss | [HF](https://huggingface.co/openai/whisper-large-v3-turbo) |
| distil-whisper/distil-large-v3 | Distilled encoder-decoder Transformer (756M) | Knowledge-distilled Whisper: 6.3x faster, within 1% WER on long-form audio | [HF](https://huggingface.co/distil-whisper/distil-large-v3) |
| Qwen/Qwen3-ASR-1.7B | AuT audio encoder (300M) + Qwen3 LLM decoder (1.7B) | SOTA open-source ASR via GSPO reinforcement learning, 52 languages, streaming + offline | [HF](https://huggingface.co/Qwen/Qwen3-ASR-1.7B) |
| espnet/yodas-granary | — | espnet | [HF](https://huggingface.co/datasets/espnet/yodas-granary) |

## Key Ideas

- Streaming ASR has reached parity with offline systems: Voxtral Realtime matches Whisper quality at 480ms delay
- Decoder-only architectures with modality-aware MoE can surpass encoder-decoder baselines without external pretrained components
- Very long context ASR (up to 1 hour) shows optimal performance at ~21 minutes, suggesting diminishing returns beyond that
- Cross-domain robustness via GAN-based adaptation (URSA-GAN) shows promise for handling noise and channel variability
